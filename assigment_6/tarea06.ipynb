{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<div style=\"background-color: lightgray; padding: 20px; color: black;\">\n",
    "<div>\n",
    "<img src=\"https://th.bing.com/th/id/R.3cd1c8dc996c5616cf6e65e20b6bf586?rik=09aaLyk4hfbBiQ&riu=http%3a%2f%2fcidics.uanl.mx%2fwp-content%2fuploads%2f2016%2f09%2fcimat.png&ehk=%2b0brgMUkA2BND22ixwLZheQrrOoYLO3o5cMRqsBOrlY%3d&risl=&pid=ImgRaw&r=0\" style=\"float: right; margin-right: 30px;\" width=\"200\"/> \n",
    "<font size=\"6.9\" color=\"8C3061\"><b>Curso de Optimización</b></font> <br>\n",
    "<font size=\"4.5\" color=\"8C3061\"><b>Tarea 6 - Gradiente Conjugado</b></font> \n",
    "</div>\n",
    "<div style=\"text-align: left\">  <br>\n",
    "Edison David Serrano Cárdenas. <br>\n",
    "MSc en Matemáticas Aplicadas <br>\n",
    "CIMAT - Sede Guanajuato <br>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"8C3061\" >**Cargar Librerías**</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from typing import Tuple, Callable\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "\n",
    "# load module Opti_functions\n",
    "from opti_functions import Opti_functions as opti\n",
    "\n",
    "# load visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"8C3061\" >**Ejercicio 1 (3.0 puntos)**</font>\n",
    "\n",
    "1. Programe el método de gradiente conjugado lineal, Algoritmo 1 de la Clase 18,\n",
    "   para resolver el sistema de ecuaciones $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$, donde\n",
    "   $\\mathbf{A}$ es una matriz simétrica y definida positiva.\n",
    "   \n",
    "   Haga que la función devuelva el último punto  $\\mathbf{x}_k$, \n",
    "   el último residual $\\mathbf{r}_k$, el número de iteraciones $k$ \n",
    "   y una variable binaria $bres$ que indique si se cumplió el criterio\n",
    "   de paro ($bres=True$) o si el algoritmo terminó por\n",
    "   iteraciones ($bres=False$).\n",
    "\n",
    "2. Pruebe el algoritmo para resolver el sistema de ecuaciones \n",
    "\n",
    "$$ \\mathbf{A}_1\\mathbf{x}=\\mathbf{b}_1$$\n",
    "\n",
    "  donde\n",
    "\n",
    "$$ \\mathbf{A}_1 = n\\mathbf{I} + \\mathbf{1} = \n",
    "\\left[\\begin{array}{llll} n      & 0      & \\cdots & 0 \\\\\n",
    "                       0      & n      & \\cdots & 0 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       0      & 0      & \\cdots & n \\end{array}\\right]\n",
    "+ \\left[\\begin{array}{llll} 1    & 1      & \\cdots & 1 \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\end{array}\\right],  \\qquad\n",
    "\\mathbf{b}_1 = \\left[\\begin{array}{l} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right], $$\n",
    "\n",
    "$n$ es la dimensión de la variable independiente\n",
    "$\\mathbf{x}=(x_1, x_2, ..., x_n)$, \n",
    "$\\mathbf{I}$ es la matriz identidad y $\\mathbf{1}$ es la matriz llena de 1's,\n",
    "ambas de tamaño $n$.\n",
    "\n",
    "También aplique el algoritmo para resolver el sistema \n",
    "\n",
    "$$ \\mathbf{A}_2\\mathbf{x}=\\mathbf{b}_2$$\n",
    "\n",
    "donde  $\\mathbf{A}_2 = [a_{ij}]$ con\n",
    "\n",
    "$$ a_{ij} = exp\\left(-0.25(i-j)^2 \\right),  \\qquad\n",
    "\\mathbf{b}_2 = \\left[\\begin{array}{l} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] $$\n",
    "\n",
    "\n",
    "- Use $\\mathbf{x}_0$ como el vector cero, el máximo número de iteraciones \n",
    "  $N=n$ y una toleracia $\\tau=\\sqrt{n} \\epsilon_m^{1/3}$,\n",
    "  donde $\\epsilon_m$ es el épsilon máquina.\n",
    "- Pruebe el algoritmo resolviendo los dos sistemas de ecuaciones con $n=10, 100, 1000$ y \n",
    "  en cada caso imprima la siguiente información\n",
    "\n",
    "- la dimensión $n$,\n",
    "- el  número $k$ de iteraciones realizadas,\n",
    "- las primeras y últimas 4 entradas del punto $\\mathbf{x}_k$ que devuelve el algoritmo,\n",
    "- la norma del residual $\\mathbf{r}_k$, \n",
    "- la variable $bres$ para saber si el algoritmo puedo converger.\n",
    "\n",
    "**Solución:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implementación del método de gradiente conjugado lineal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(x0:np.ndarray, \n",
    "                       A:np.ndarray,\n",
    "                       b:np.ndarray, \n",
    "                       max_iter:int, \n",
    "                       tol:float) -> Tuple[np.ndarray, np.ndarray, int, bool]:\n",
    "    \"\"\"\n",
    "    Conjugate gradient method to solve the linear system Ax = b\n",
    "    \n",
    "    :param x0: initial guess (np.ndarray)\n",
    "    :param A: matrix A (np.ndarray)\n",
    "    :param b: vector b (np.ndarray)\n",
    "    :param max_iter: maximum number of iterations (int)\n",
    "    :param tol: tolerance (float)\n",
    "    \n",
    "    :return: Tuple with the solution of the linear system, the residual, the number of iterations and a boolean indicating if the method converges\n",
    "    \"\"\"\n",
    "    \n",
    "    r0 = A @ x0 - b\n",
    "    p0 = -r0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        if np.linalg.norm(r0) < tol:\n",
    "            return x0, r0, i, True\n",
    "        alpha = r0.T@r0 / (p0.T @ A @ p0)\n",
    "        xk = x0 + alpha * p0\n",
    "        rk = r0 + alpha * A @ p0\n",
    "        beta = (rk.T @ rk) / (r0.T @ r0)\n",
    "        pk = -rk + beta * p0\n",
    "        r0 = rk\n",
    "        p0 = pk\n",
    "        x0 = xk\n",
    "    return x0, r0, i, False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Evaluación del algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix1(n:int) -> np.ndarray:\n",
    "    return n*np.eye(n) + np.ones((n,n))\n",
    "\n",
    "def matrix2(n:int) -> np.ndarray:\n",
    "    return np.fromfunction(lambda i, j: np.exp(-0.25*(i - j)**2), (n, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testear el algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_conjugate_gradient(n:int,matrix:callable) -> None:\n",
    "    \"\"\"\n",
    "    Test the conjugate gradient method\n",
    "    \n",
    "    :param n: size of the matrix\n",
    "    \"\"\"\n",
    "    x0 = np.zeros(n)\n",
    "    tol  = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "    xk, rk, n_iter, success = conjugate_gradient(x0, matrix(n), np.ones(n), n, tol)\n",
    "\n",
    "    print(f\"Dimensión:\\t\\t{n}\")\n",
    "    print(f\"Número de iteraciones:\\t{n_iter}\")\n",
    "    print(\"Solución:\\t\\t\",','.join(map(str, xk[:4])),\",...,\",','.join(map(str, xk[-4:])))\n",
    "    print(\"Convergencia:\\t\\t\", success,\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matriz 1:** Los resultados siguientes son esperados debido a que el vector $\\mathbf{b}_1$ es un vector propio de $\\mathbf{A}_1$, luego, el algoritmo converge en una iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t10\n",
      "Número de iteraciones:\t1\n",
      "Solución:\t\t 0.05,0.05,0.05,0.05 ,..., 0.05,0.05,0.05,0.05\n",
      "Convergencia:\t\t True \n",
      "\n",
      "Dimensión:\t\t100\n",
      "Número de iteraciones:\t1\n",
      "Solución:\t\t 0.005,0.005,0.005,0.005 ,..., 0.005,0.005,0.005,0.005\n",
      "Convergencia:\t\t True \n",
      "\n",
      "Dimensión:\t\t1000\n",
      "Número de iteraciones:\t1\n",
      "Solución:\t\t 0.0005,0.0005,0.0005,0.0005 ,..., 0.0005,0.0005,0.0005,0.0005\n",
      "Convergencia:\t\t True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_conjugate_gradient(10,matrix=matrix1)\n",
    "test_conjugate_gradient(100,matrix=matrix1)\n",
    "test_conjugate_gradient(1000,matrix=matrix1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matriz 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t10\n",
      "Número de iteraciones:\t5\n",
      "Solución:\t\t 1.3690991585471464,-1.166376817178349,1.6090828050291892,-0.6133905258859524 ,..., -0.6133905258860959,1.6090828050289954,-1.1663768171785018,1.3690991585470598\n",
      "Convergencia:\t\t True \n",
      "\n",
      "Dimensión:\t\t100\n",
      "Número de iteraciones:\t99\n",
      "Solución:\t\t 1.446307563888463,-1.4163725105090534,2.1104266146806023,-1.4249577511636988 ,..., -1.4249578827801062,2.110458023278651,-1.4163439459655913,1.446290949265481\n",
      "Convergencia:\t\t False \n",
      "\n",
      "Dimensión:\t\t1000\n",
      "Número de iteraciones:\t262\n",
      "Solución:\t\t 1.446288242264973,-1.4163595431324436,2.11051809682915,-1.4250723051291694 ,..., -1.4250723051291925,2.110518096829142,-1.4163595431324532,1.4462882422649672\n",
      "Convergencia:\t\t True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_conjugate_gradient(10,matrix=matrix2)\n",
    "test_conjugate_gradient(100,matrix=matrix2)\n",
    "test_conjugate_gradient(1000,matrix=matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"8C3061\" >**Ejercicio 2 (3.5 puntos)**</font>\n",
    "\n",
    "Programar el método de gradiente conjugado no lineal descrito en el Algoritmo 3 \n",
    "de Clase 19 usando la fórmula de Fletcher-Reeves:\n",
    "\n",
    "$$ \\beta_{k+1} = \\frac{\\nabla f_{k+1}^\\top \\nabla f_{k+1}}{\\nabla f_{k}^\\top\\nabla f_{k}}  $$ \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "1. Escriba la función que implemente el algoritmo. \n",
    "\n",
    "- La función debe recibir como argumentos $\\mathbf{x}_0$, la función $f$ y \n",
    "  su gradiente, el número máximo de iteraciones $N$, la tolerancia $\\tau$, y los\n",
    "  parámetros para el algoritmo de backtracking: factor $\\rho$, la constante $c_1$\n",
    "  para la condición de descenso suficiente, la constante $c_2$ para la condición\n",
    "  de curvatura, y el máximo número de iteraciones $N_b$.\n",
    "- Agregue al algoritmo un contador\n",
    "  $nr$ que se incremente cada vez que se aplique el reinicio, es decir, cuando\n",
    "  se hace $\\beta_{k+1}=0$.\n",
    "   \n",
    "- Para calcular el tamaño de paso $\\alpha_k$ use el algoritmo de backtracking\n",
    "  usando las condiciones de Wolfe con el valor inicial $\\alpha_{ini}=1$.\n",
    "\n",
    "- Haga que la función devuelva el último punto  $\\mathbf{x}_k$, \n",
    "  el último gradiente $\\mathbf{g}_k$, el número de iteraciones $k$ \n",
    "  y una variable binaria $bres$ que indique si se cumpli\\'o el criterio\n",
    "  de paro ($bres=True$) o si el algoritmo terminó por\n",
    "  iteraciones ($bres=False$), y el contador $br·.\n",
    "\n",
    "2. Pruebe el algoritmo usando la siguientes funciones con los puntos iniciales dados:\n",
    "\n",
    "\n",
    "**Función de cuadrática 1:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "- $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top\\mathbf{A}_1\\mathbf{x} - \\mathbf{b}_1^\\top\\mathbf{x}$,\n",
    "  donde $\\mathbf{A}_1$ y $\\mathbf{b}_1$ están definidas como en el Ejercicio 1.\n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{10}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{100}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{1000}$ \n",
    "\n",
    "**Función de cuadrática 2:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "- $f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top\\mathbf{A}_2\\mathbf{x} - \\mathbf{b}_2^\\top\\mathbf{x}$,\n",
    "  donde $\\mathbf{A}_2$ y $\\mathbf{b}_2$ están definidas como en el Ejercicio 1.\n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{10}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{100}$ \n",
    "- $\\mathbf{x}_0 = (0,...,0)\\in \\mathbb{R}^{1000}$ \n",
    "\n",
    "**Función de Beale :** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$f(\\mathbf{x}) = (1.5-x_1 + x_1x_2)^2 + (2.25 - x_1 + x_1x_2^2)^2 + (2.625 - x_1 + x_1x_2^3)^2.$$\n",
    "- $\\mathbf{x}_0 = (2,3)$  \n",
    "   \n",
    "\n",
    "**Función de Himmelblau:** Para $\\mathbf{x}=(x_1,x_2)$\n",
    "\n",
    "$$f(\\mathbf{x}) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2. $$\n",
    "- $\\mathbf{x}_0 = (2,4)$\n",
    "\n",
    "\n",
    "\n",
    "**Función de Rosenbrock:** Para $\\mathbf{x}=(x_1,x_2, ..., x_n)$\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\sum_{i=1}^{n-1} \\left[100(x_{i+1} - x_i^2)^2 + (1-x_i)^2 \\right]\n",
    "\\quad n\\geq 2.$$\n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0)\\in \\mathbb{R}^{2}$  \n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0, ..., -1.2, 1.0) \\in \\mathbb{R}^{20}$  \n",
    "- $\\mathbf{x}_0 = (-1.2, 1.0, ..., -1.2, 1.0) \\in \\mathbb{R}^{40}$ \n",
    "\n",
    "\n",
    "3. Fije $N=5000$, $\\tau = \\sqrt{n}\\epsilon_m^{1/3}$, donde $n$ es la dimensión\n",
    "   de la variable $\\mathbf{x}$ y $\\epsilon_m$ es el épsilon máquina. \n",
    "   Para backtracking use $\\rho=0.5$, $c_1=0.001$, $c_2=0.01$, $N_b=500$.\n",
    "   \n",
    "4. Para cada función de prueba imprima\n",
    "   \n",
    "- la dimensión $n$,\n",
    "- $f(\\mathbf{x}_0)$,\n",
    "- el  número $k$ de iteraciones realizadas,\n",
    "- $f(\\mathbf{x}_k)$,\n",
    "- las primeras y últimas 4 entradas del punto $\\mathbf{x}_k$ que devuelve el algoritmo,\n",
    "- la norma del vector gradiente $\\mathbf{g}_k$, \n",
    "- la variable $bres$ para saber si el algoritmo puedo converger.\n",
    "- el número de reinicios $nr$.\n",
    "  \n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_conjugate_gradient( x0:np.ndarray, \n",
    "                                  f:Callable[[np.ndarray], float],\n",
    "                                  grad_f:Callable[[np.ndarray], np.ndarray],\n",
    "                                  max_iter:int,\n",
    "                                  tol:float,\n",
    "                                  rho:float=0.5,\n",
    "                                  c1:float=1e-4, \n",
    "                                  c2:float=0.9,\n",
    "                                  max_iter_b:int=100,\n",
    "                                  alt = False) -> Tuple[np.ndarray, np.ndarray, int, bool]: \n",
    "                                  \n",
    "    \"\"\"\n",
    "    Conjugate gradient method to find the minimum of a function\n",
    "    \n",
    "    :param x0: initial guess (np.ndarray)\n",
    "    :param f: function to minimize (Callable[[np.ndarray], float])\n",
    "    :param grad_f: gradient of the function (Callable[[np.ndarray], np.ndarray])\n",
    "    :param max_iter: maximum number of iterations (int)\n",
    "    :param tol: tolerance (float)\n",
    "    :param rho: parameter for the backtracking line search (float)\n",
    "    :param c1: parameter for the Armijo condition (float)\n",
    "    :param c2: parameter for the Wolfe condition (float)\n",
    "    :param max_iter_b: maximum number of iterations for the backtracking line search (int)\n",
    "    \n",
    "    :return: Tuple with the minimum of the function, the gradient at the minimum, the number of iterations and a boolean indicating if the method converges\n",
    "    \"\"\"\n",
    "    \n",
    "    br = 0\n",
    "    \n",
    "    g0 = grad_f(x0)\n",
    "    d0 = -g0\n",
    "    for i in range(max_iter):\n",
    "        if np.linalg.norm(g0) < tol:\n",
    "            return x0, g0, i, True, br\n",
    "        alpha,_ = opti.brack_tracking_Wolfe(alpha_init=1, rho=rho, c1=c1, c2=c2, xk=x0, f=f, gradf=grad_f, fk=f(x0), grad_fk=grad_f(x0), dir_pk=d0,iter_maxb=max_iter_b,alt=alt)\n",
    "        xk = x0 + alpha * d0\n",
    "        gk = grad_f(xk)\n",
    "        if abs(gk.T@g0) < 0.2*gk.T@gk:\n",
    "            beta = (gk.T @ gk) / (g0.T@g0)\n",
    "        else: \n",
    "            beta = 0\n",
    "            br+=1   \n",
    "        dk = -gk + beta * d0\n",
    "        x0 = xk\n",
    "        g0 = gk\n",
    "        d0 = dk\n",
    "    return x0, g0, i, False, br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Testear el algoritmo en las funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_non_linear_conjugate_gradient(f:Callable[[np.ndarray], float],\n",
    "                                        grad_f:Callable[[np.ndarray], np.ndarray],\n",
    "                                        x0:np.ndarray, \n",
    "                                        max_iter:int, \n",
    "                                        tol:float, \n",
    "                                        rho:float, \n",
    "                                        c1:float, \n",
    "                                        c2:float, \n",
    "                                        N_b:int,\n",
    "                                        alt:bool=False) -> None:\n",
    "    xk, grad_xk, iterk, conv, br = nonlinear_conjugate_gradient(x0,f, grad_f, max_iter, tol, rho, c1, c2, N_b,alt)\n",
    "\n",
    "    print(\"Dimensión:\\t\\t\",len(x0))\n",
    "    print(\"f(x0):\\t\\t\\t\",f(x0))\n",
    "    print(\"Número de iteraciones:\\t\",iterk)\n",
    "    print(\"f(xk):\\t\\t\\t\",f(xk))\n",
    "    if n>4:\n",
    "        print(\"xk:\\t\\t\",','.join(map(str, xk[:4])),\",...,\",','.join(map(str, xk[-4:])))\n",
    "    else:\n",
    "        print(\"xk:\\t\\t\\t\",xk)\n",
    "    print(\"||grad_f(x_k)||: \\t\",np.linalg.norm(grad_xk))\n",
    "    print(\"Convergencia:\\t\\t\",conv)\n",
    "    print(\"nr:\\t\\t\\t\",br)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 5000 \n",
    "rho = 0.5\n",
    "c1 = 1e-3\n",
    "c2 = 0.01\n",
    "N_b = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función Cuadratica 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A11 = matrix1(10)\n",
    "A12 = matrix1(100)\n",
    "A13 = matrix1(1000)\n",
    "b1 = np.ones(10)\n",
    "b2 = np.ones(100)\n",
    "b3 = np.ones(1000) \n",
    "\n",
    "f_square11 = lambda x: 0.5*x.T@A11@x-b1@x\n",
    "gradf_square11 = lambda x: A11@x-b1\n",
    "\n",
    "f_square12 = lambda x: 0.5*x.T@A12@x-b2@x\n",
    "gradf_square12 = lambda x: A12@x-b2\n",
    "\n",
    "f_square13 = lambda x: 0.5*x.T@A13@x-b3@x\n",
    "gradf_square13 = lambda x: A13@x-b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 10\n",
      "f(x0):\t\t\t 0.0\n",
      "Número de iteraciones:\t 9\n",
      "f(xk):\t\t\t -0.24999999999636202\n",
      "xk:\t\t 0.05000019073486328,0.05000019073486328,0.05000019073486328,0.05000019073486328 ,..., 0.05000019073486328,0.05000019073486328,0.05000019073486328,0.05000019073486328\n",
      "||grad_f(x_k)||: \t 1.206313194339134e-05\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 9\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros(10)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient(f_square11,gradf_square11,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 100\n",
      "f(x0):\t\t\t 0.0\n",
      "Número de iteraciones:\t 21\n",
      "f(xk):\t\t\t -0.2499999999919999\n",
      "xk:\t\t 0.005000028284145768,0.005000028284145768,0.005000028284145768,0.005000028284145768 ,..., 0.005000028284145769,0.005000028284145769,0.005000028284145769,0.005000028284145769\n",
      "||grad_f(x_k)||: \t 5.6568291540037836e-05\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 21\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros(100)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient(f_square12,gradf_square12,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 1000\n",
      "f(x0):\t\t\t 0.0\n",
      "Número de iteraciones:\t 251\n",
      "f(xk):\t\t\t -0.2499999999914645\n",
      "xk:\t\t 0.0005000029213602828,0.0005000029213602828,0.0005000029213602828,0.0005000029213602828 ,..., 0.0005000029213602874,0.0005000029213602885,0.0005000029213602883,0.0005000029213602873\n",
      "||grad_f(x_k)||: \t 0.00018476304752628408\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 251\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros(1000)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient(f_square13,gradf_square13,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función Cuadratica 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A11 = matrix2(10)\n",
    "A12 = matrix2(100)\n",
    "A13 = matrix2(1000)\n",
    "b1 = np.ones(10)\n",
    "b2 = np.ones(100)\n",
    "b3 = np.ones(1000)\n",
    "\n",
    "f_square11 = lambda x: 0.5*x.T@A11@x-b1@x\n",
    "gradf_square11 = lambda x: A11@x-b1\n",
    "\n",
    "f_square12 = lambda x: 0.5*x.T@A12@x-b2@x\n",
    "gradf_square12 = lambda x: A12@x-b2\n",
    "\n",
    "f_square13 = lambda x: 0.5*x.T@A13@x-b3@x\n",
    "gradf_square13 = lambda x: A13@x-b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 10\n",
      "f(x0):\t\t\t 0.0\n",
      "Número de iteraciones:\t 1571\n",
      "f(xk):\t\t\t -1.7934207913526619\n",
      "xk:\t\t 1.3688956634167995,-1.165862919253239,1.6083890463638972,-0.6127911466676729 ,..., -0.6127911466676721,1.6083890463638963,-1.1658629192532382,1.3688956634167995\n",
      "||grad_f(x_k)||: \t 1.895048303749792e-05\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 1230\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros(10)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient(f_square11,gradf_square11,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 100\n",
      "f(x0):\t\t\t 0.0\n",
      "Número de iteraciones:\t 4999\n",
      "f(xk):\t\t\t -14.494237120734711\n",
      "xk:\t\t 1.4394380697559654,-1.3950315645431932,2.069993316201003,-1.3637847328940806 ,..., -1.3637847328940682,2.0699933162009954,-1.3950315645431886,1.4394380697559637\n",
      "||grad_f(x_k)||: \t 0.0010685994764851967\n",
      "Convergencia:\t\t False\n",
      "nr:\t\t\t 4981\n"
     ]
    }
   ],
   "source": [
    "rho = 0.6\n",
    "x0 = np.zeros(100)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient(f_square12,gradf_square12,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 1000\n",
      "f(x0):\t\t\t 0.0\n",
      "Número de iteraciones:\t 2590\n",
      "f(xk):\t\t\t -141.4369672302889\n",
      "xk:\t\t 1.4436347652581003,-1.4080703063030438,2.0946540196166907,-1.4008840724651848 ,..., -1.400884072465163,2.0946540196166756,-1.4080703063030353,1.4436347652580972\n",
      "||grad_f(x_k)||: \t 0.00018992149007912416\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 2103\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros(1000)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "rho = 0.5\n",
    "print_non_linear_conjugate_gradient(f_square13,gradf_square13,x0,max_iter,tol,rho,c1,c2,N_b,alt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función Himmelblau**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 2\n",
      "f(x0):\t\t\t 130.0\n",
      "Número de iteraciones:\t 31\n",
      "f(xk):\t\t\t 2.4720276275992285e-13\n",
      "xk:\t\t\t [ 3.58442828 -1.84812647]\n",
      "||grad_f(x_k)||: \t 6.528645259773077e-06\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 31\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2.0,4.0])\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient(opti.fncHimmelblau,opti.grad_fncHimmelblau,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función Beale:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 2\n",
      "f(x0):\t\t\t 3347.203125\n",
      "Número de iteraciones:\t 1619\n",
      "f(xk):\t\t\t 2.2029216391375518e-11\n",
      "xk:\t\t\t [3.00001161 0.50000274]\n",
      "||grad_f(x_k)||: \t 8.380681550427131e-06\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 1619\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2.0,3.0])\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient(opti.fncBeale, opti.grad_fncBeale,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función Rosenbrock** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 2\n",
      "f(x0):\t\t\t 24.199999999999996\n",
      "Número de iteraciones:\t 4999\n",
      "f(xk):\t\t\t 7.919067457475048e-10\n",
      "xk:\t\t\t [1.00002802 1.0000563 ]\n",
      "||grad_f(x_k)||: \t 7.000359375329152e-05\n",
      "Convergencia:\t\t False\n",
      "nr:\t\t\t 4248\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2,1.0])\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient(opti.fncRosenbrock, opti.grad_fncRosenbrock,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 20\n",
      "f(x0):\t\t\t 4598.0\n",
      "Número de iteraciones:\t 860\n",
      "f(xk):\t\t\t 1.0453665104406672e-12\n",
      "xk:\t\t 1.0000000045877049,1.0000000053488944,1.0000000097468837,1.000000008128462 ,..., 1.000000180660494,1.000000349774317,1.000000699531955,1.0000013983356322\n",
      "||grad_f(x_k)||: \t 2.4641847474138866e-05\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 692\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2,1.0]*10)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient(opti.fncRosenbrock, opti.grad_fncRosenbrock,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 40\n",
      "f(x0):\t\t\t 9680.000000000004\n",
      "Número de iteraciones:\t 2337\n",
      "f(xk):\t\t\t 1.4191714098163482e-10\n",
      "xk:\t\t 0.9999999997589897,1.0000000006125462,0.9999999990394711,1.000000001327496 ,..., 1.0000025616005486,1.0000051384392061,1.0000103005292058,1.000020653404749\n",
      "||grad_f(x_k)||: \t 3.598898991128314e-05\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 2228\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2,1.0]*20)\n",
    "n = len(x0)\n",
    "rho = 0.6\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient(opti.fncRosenbrock, opti.grad_fncRosenbrock,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"8C3061\" >**Ejercicio 3 (3.5 puntos)**</font>\n",
    "\n",
    "Programar el método de gradiente conjugado no lineal de usando la fórmula de\n",
    "Hestenes-Stiefel:\n",
    "\n",
    "En este caso el algoritmo es igual al del Ejercicio 2, con excepción del cálculo de $\\beta_{k+1}$. Primero se calcula el vector $\\mathbf{y}_k$ y luego $\\beta_{k+1}$:\n",
    "\n",
    "$$ \\mathbf{y}_k =  \\nabla f_{k+1}-\\nabla f_{k} $$\n",
    "$$ \\beta_{k+1} =   \\frac{\\nabla f_{k+1}^\\top\\mathbf{y}_k }{\\nabla p_{k}^\\top\\mathbf{y}_k}  $$\n",
    "\n",
    "1. Repita el Ejercicio 2 usando la fórmula de Hestenes-Stiefel.\n",
    "2. ¿Hay alguna diferencia que indique que es mejor usar la fórmula de Hestenes-Stiefel\n",
    "   respesto a Fletcher-Reeves?\n",
    "3. La cantidad de reinicios puede indicar que tanto se comporta el algoritmo\n",
    "   como el algoritmo de descenso máximo. Agregue un comentario sobre esto \n",
    "   de acuerdo a los resultados obtenidos para cada fórmula.\n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Nota:</b> La implementación de backtraking se realizó de dos maneras, una en la que busca encontrar el tamaño de paso tal que se satisfaga las condiciones de Wolfe, como es indicado en la tarea. Además, se considera una alternativa que busca hallar paso de salto que cumpla la condición de descenso suficiente y la de curvatura, pero, en caso de no cumplir la de curvatura, seleccione el primer tamaño de paso que cumple Armijo con el fin de no dar pasos tan pequeños con frecuencia.\n",
    "\n",
    "Además, con el fin de que el código corriera más rápido se asignó $\\rho=0.6$, con el fin de que aumentase la probabilidad de no recorrer todo el algoritmo de backtracking, ya que $alpha$ tiene más puntos para analizar que puedan cumplir las condiciones de curvatura.  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_conjugate_gradient2(x0:np.ndarray, \n",
    "                                  f:Callable[[np.ndarray], float],\n",
    "                                    grad_f:Callable[[np.ndarray], np.ndarray],\n",
    "                                    max_iter:int,\n",
    "                                    tol:float,\n",
    "                                    rho:float=0.5,\n",
    "                                    c1:float=1e-4,\n",
    "                                    c2:float=0.9,\n",
    "                                    max_iter_b:int=100,\n",
    "                                    alt = False) -> Tuple[np.ndarray, np.ndarray, int, bool, int]: \n",
    "                                  \n",
    "    \"\"\"\n",
    "    Conjugate gradient method to find the minimum of a function\n",
    "    \n",
    "    :param x0: initial guess\n",
    "    :param f: function to minimize\n",
    "    :param grad_f: gradient of the function\n",
    "    :param max_iter: maximum number of iterations\n",
    "    :param tol: tolerance\n",
    "    :param rho: parameter for the backtracking line search\n",
    "    :param c1: parameter for the Armijo condition\n",
    "    :param c2: parameter for the Wolfe condition\n",
    "    :param max_iter_b: maximum number of iterations for the backtracking line search\n",
    "    \n",
    "    :return: Tuple with the minimum of the function, the gradient at the minimum, the number of iterations and a boolean indicating if the method converges\n",
    "    \"\"\"\n",
    "    br = 0\n",
    "    \n",
    "    g0 = grad_f(x0)\n",
    "    d0 = -g0\n",
    "    for i in range(max_iter):\n",
    "        if np.linalg.norm(g0) < tol:\n",
    "            return x0, g0, i, True, br\n",
    "        \n",
    "        alpha,_ = opti.brack_tracking_Wolfe(alpha_init=1, rho=rho, c1=c1, c2=c2, xk=x0, f=f, gradf=grad_f, fk=f(x0), grad_fk=grad_f(x0), dir_pk=d0,iter_maxb=max_iter_b,alt=alt)\n",
    "        xk = x0 + alpha * d0\n",
    "        gk = grad_f(xk)\n",
    "        if abs(gk.T@g0) < 0.2*gk.T@gk:\n",
    "            yk = gk - g0\n",
    "            beta = (gk.T @ yk) / (d0.T @ yk) \n",
    "        else: \n",
    "            beta = 0\n",
    "            br+=1   \n",
    "        dk = -gk + beta * d0\n",
    "        x0 = xk\n",
    "        g0 = gk\n",
    "        d0 = dk\n",
    "    return x0, g0, i, False, br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_non_linear_conjugate_gradient2(f:Callable[[np.ndarray], float],\n",
    "                                        grad_f:Callable[[np.ndarray], np.ndarray],\n",
    "                                        x0:np.ndarray, \n",
    "                                        max_iter:int, \n",
    "                                        tol:float, \n",
    "                                        rho:float, \n",
    "                                        c1:float, \n",
    "                                        c2:float, \n",
    "                                        N_b:int,\n",
    "                                        alt:bool=False) -> None:\n",
    "    xk, grad_xk, iterk, conv, br = nonlinear_conjugate_gradient2(x0,f, grad_f, max_iter, tol, rho, c1, c2, N_b,alt)\n",
    "\n",
    "    print(\"Dimensión:\\t\\t\",len(x0))\n",
    "    print(\"f(x0):\\t\\t\\t\",f(x0))\n",
    "    print(\"Número de iteraciones:\\t\",iterk)\n",
    "    print(\"f(xk):\\t\\t\\t\",f(xk))\n",
    "    if n>4:\n",
    "        print(\"xk:\\t\\t\",','.join(map(str, xk[:4])),\",...,\",','.join(map(str, xk[-4:])))\n",
    "    else:\n",
    "        print(\"xk:\\t\\t\\t\",xk)\n",
    "    print(\"||grad_f(x_k)||: \\t\",np.linalg.norm(grad_xk))\n",
    "    print(\"Convergencia:\\t\\t\",conv)\n",
    "    print(\"nr:\\t\\t\\t\",br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 5000 \n",
    "rho = 0.5\n",
    "c1 = 1e-3\n",
    "c2 = 0.01\n",
    "N_b = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función Cuadratica 1**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "A11 = matrix1(10)\n",
    "A12 = matrix1(100)\n",
    "A13 = matrix1(1000)\n",
    "b1 = np.ones(10)\n",
    "b2 = np.ones(100)\n",
    "b3 = np.ones(1000) \n",
    "f_square11 = lambda x: 0.5*x.T@A11@x-b1@x\n",
    "gradf_square11 = lambda x: A11@x-b1\n",
    "\n",
    "f_square12 = lambda x: 0.5*x.T@A12@x-b2@x\n",
    "gradf_square12 = lambda x: A12@x-b2\n",
    "\n",
    "f_square13 = lambda x: 0.5*x.T@A13@x-b3@x\n",
    "gradf_square13 = lambda x: A13@x-b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 10\n",
      "f(x0):\t\t\t 0.0\n",
      "Número de iteraciones:\t 9\n",
      "f(xk):\t\t\t -0.24999999999636202\n",
      "xk:\t\t 0.05000019073486328,0.05000019073486328,0.05000019073486328,0.05000019073486328 ,..., 0.05000019073486328,0.05000019073486328,0.05000019073486328,0.05000019073486328\n",
      "||grad_f(x_k)||: \t 1.206313194339134e-05\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 9\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros(10)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient2(f_square11,gradf_square11,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 100\n",
      "f(x0):\t\t\t 0.0\n",
      "Número de iteraciones:\t 21\n",
      "f(xk):\t\t\t -0.2499999999919999\n",
      "xk:\t\t 0.005000028284145768,0.005000028284145768,0.005000028284145768,0.005000028284145768 ,..., 0.005000028284145769,0.005000028284145769,0.005000028284145769,0.005000028284145769\n",
      "||grad_f(x_k)||: \t 5.6568291540037836e-05\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 21\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros(100)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient2(f_square12,gradf_square12,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 1000\n",
      "f(x0):\t\t\t 0.0\n",
      "Número de iteraciones:\t 251\n",
      "f(xk):\t\t\t -0.2499999999914645\n",
      "xk:\t\t 0.0005000029213602828,0.0005000029213602828,0.0005000029213602828,0.0005000029213602828 ,..., 0.0005000029213602874,0.0005000029213602885,0.0005000029213602883,0.0005000029213602873\n",
      "||grad_f(x_k)||: \t 0.00018476304752628408\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 251\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros(1000)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient2(f_square13,gradf_square13,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función Cuadratica 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "A11 = matrix2(10)\n",
    "A12 = matrix2(100)\n",
    "A13 = matrix2(1000)\n",
    "b1 = np.ones(10)\n",
    "b2 = np.ones(100)\n",
    "b3 = np.ones(1000)\n",
    "\n",
    "f_square11 = lambda x: 0.5*x.T@A11@x-b1@x\n",
    "gradf_square11 = lambda x: A11@x-b1\n",
    "\n",
    "f_square12 = lambda x: 0.5*x.T@A12@x-b2@x\n",
    "gradf_square12 = lambda x: A12@x-b2\n",
    "\n",
    "f_square13 = lambda x: 0.5*x.T@A13@x-b3@x\n",
    "gradf_square13 = lambda x: A13@x-b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 10\n",
      "f(x0):\t\t\t 0.0\n",
      "Número de iteraciones:\t 1571\n",
      "f(xk):\t\t\t -1.7934207913526619\n",
      "xk:\t\t 1.3688956634167995,-1.165862919253239,1.6083890463638972,-0.6127911466676729 ,..., -0.6127911466676721,1.6083890463638963,-1.1658629192532382,1.3688956634167995\n",
      "||grad_f(x_k)||: \t 1.895048303749792e-05\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 1230\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros(10)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient2(f_square11,gradf_square11,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 100\n",
      "f(x0):\t\t\t 0.0\n",
      "Número de iteraciones:\t 4999\n",
      "f(xk):\t\t\t -14.494237120734711\n",
      "xk:\t\t 1.4394380697559654,-1.3950315645431932,2.069993316201003,-1.3637847328940806 ,..., -1.3637847328940682,2.0699933162009954,-1.3950315645431886,1.4394380697559637\n",
      "||grad_f(x_k)||: \t 0.0010685994764851967\n",
      "Convergencia:\t\t False\n",
      "nr:\t\t\t 4981\n"
     ]
    }
   ],
   "source": [
    "rho = 0.6\n",
    "x0 = np.zeros(100)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient2(f_square12,gradf_square12,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 1000\n",
      "f(x0):\t\t\t 0.0\n",
      "Número de iteraciones:\t 4999\n",
      "f(xk):\t\t\t -141.43691110446827\n",
      "xk:\t\t 1.4404743792219015,-1.398298401648737,2.076050769403433,-1.3728570552095603 ,..., -1.3728570552095452,2.076050769403422,-1.3982984016487308,1.4404743792219\n",
      "||grad_f(x_k)||: \t 0.0005855903072393323\n",
      "Convergencia:\t\t False\n",
      "nr:\t\t\t 3720\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros(1000)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "rho = 0.5\n",
    "print_non_linear_conjugate_gradient2(f_square13,gradf_square13,x0,max_iter,tol,rho,c1,c2,N_b,alt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función Himmelblau**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 2\n",
      "f(x0):\t\t\t 130.0\n",
      "Número de iteraciones:\t 37\n",
      "f(xk):\t\t\t 1.9833788488942771e-13\n",
      "xk:\t\t\t [ 3.58442828 -1.84812653]\n",
      "||grad_f(x_k)||: \t 6.466611484709132e-06\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 36\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2.0,4.0])\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient2(opti.fncHimmelblau,opti.grad_fncHimmelblau,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función Beale:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 2\n",
      "f(x0):\t\t\t 3347.203125\n",
      "Número de iteraciones:\t 769\n",
      "f(xk):\t\t\t 5.6113870648033834e-11\n",
      "xk:\t\t\t [3.00001871 0.50000457]\n",
      "||grad_f(x_k)||: \t 7.496323664379619e-06\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 585\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2.0,3.0])\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient2(opti.fncBeale, opti.grad_fncBeale,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función Rosenbrock** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 2\n",
      "f(x0):\t\t\t 24.199999999999996\n",
      "Número de iteraciones:\t 1382\n",
      "f(xk):\t\t\t 1.606990811512821e-11\n",
      "xk:\t\t\t [0.999996   0.99999198]\n",
      "||grad_f(x_k)||: \t 8.384827957465905e-06\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 1128\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2,1.0])\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient2(opti.fncRosenbrock, opti.grad_fncRosenbrock,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 20\n",
      "f(x0):\t\t\t 4598.0\n",
      "Número de iteraciones:\t 1615\n",
      "f(xk):\t\t\t 8.876616970657303e-11\n",
      "xk:\t\t 1.0000000032570238,1.0000000115016339,1.0000000004616163,1.0000000105180396 ,..., 1.0000020219610348,1.0000040577194274,1.0000081453807228,1.0000163324635707\n",
      "||grad_f(x_k)||: \t 2.657427171845461e-05\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 1265\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2,1.0]*10)\n",
    "n = len(x0)\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient2(opti.fncRosenbrock, opti.grad_fncRosenbrock,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión:\t\t 40\n",
      "f(x0):\t\t\t 9680.000000000004\n",
      "Número de iteraciones:\t 1347\n",
      "f(xk):\t\t\t 2.6581720338929803e-11\n",
      "xk:\t\t 1.0000000002214249,0.9999999994435402,1.0000000008515648,0.9999999988722171 ,..., 1.000001080190785,1.0000021798670349,1.0000043859552445,1.0000087995265188\n",
      "||grad_f(x_k)||: \t 3.349524293049548e-05\n",
      "Convergencia:\t\t True\n",
      "nr:\t\t\t 1222\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([-1.2,1.0]*20)\n",
    "n = len(x0)\n",
    "rho = 0.6\n",
    "tol = np.sqrt(n)*(np.finfo(float).eps)**(1/3)\n",
    "print_non_linear_conjugate_gradient2(opti.fncRosenbrock, opti.grad_fncRosenbrock,x0,max_iter,tol,rho,c1,c2,N_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. No se evidencia alguna mejora o empeoramiento usando ambas formulas, ya que bajo unas funciones funciona mejor una formula que la otra pero no se evidencia una mejora substancial. Además que se observa que bajo estos algoritmos son bastantes sensibles a la parametrización del backtracking con las condiciones de Wolfe.\n",
    "\n",
    "2.  El número de reinicios indica que en la mayor parte del tiempo el algoritmo se comporta como el algoritmo de descenso máximo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
